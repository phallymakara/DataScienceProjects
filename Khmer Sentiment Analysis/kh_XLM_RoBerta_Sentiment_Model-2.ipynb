{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Enhancing Khmer Sentiment Analysis Using Transformer Models"
      ],
      "metadata": {
        "id": "LepvqXKEXLNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies"
      ],
      "metadata": {
        "id": "4i2nL23lM8fl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For Word Segmentation"
      ],
      "metadata": {
        "id": "Yz3vxBacNHCT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pN0dPp5GMHLY"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/rinabuoy/KhmerNLP"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd KhmerNLP"
      ],
      "metadata": {
        "id": "mvofD0izNlRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn_crfsuite"
      ],
      "metadata": {
        "id": "yoeKQCDfNq7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import TFAutoModel,AutoTokenizer,BertModel,AdamW,XLMRobertaTokenizer,XLMRobertaForSequenceClassification, Trainer, TrainingArguments,XLMRobertaModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "#from khmerwordsegmentor import KhmerWordSegmentor\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "mjyRjUoTNvOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data and Preprocessing"
      ],
      "metadata": {
        "id": "YGqR7f6Xyp_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/kh_sentiment_data_segmented (1).csv')"
      ],
      "metadata": {
        "id": "loeFJq8VOZGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(10)"
      ],
      "metadata": {
        "id": "c0aAKc43y5c9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KdUVSenJr8PU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isna().sum() #missing value"
      ],
      "metadata": {
        "id": "h8YlCW1DzB8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Sentiment_label'].value_counts() # Check class label, prevent from imbalance class"
      ],
      "metadata": {
        "id": "2W8vGKI4zId3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Segmentation"
      ],
      "metadata": {
        "id": "Vxs2VAvr1KLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the segmentor\n",
        "seg = KhmerWordSegmentor()\n",
        "def segment_khmer_text(text, model=\"lstm\"):\n",
        "    \"\"\"\n",
        "    Segment Khmer text using the specified model (LSTM or CRF).\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The input Khmer text to be segmented.\n",
        "        model (str): The model to use for segmentation ('lstm' or 'crf').\n",
        "\n",
        "    Returns:\n",
        "        str: The segmented text as a string.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Segment the text\n",
        "        segmented_text = seg.segment(text, model=model)\n",
        "        return segmented_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error during segmentation: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "_rUIyrwbzUuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['seg_text_lstm'] = data['Text'].apply(lambda x: segment_khmer_text(x, model='lstm')) # apply word segmentation using \"LSTM\"\n",
        "data['seg_text_crf'] = data['Text'].apply(lambda x: segment_khmer_text(x,model = 'crf')) # apply word segmentation using \"CRF\""
      ],
      "metadata": {
        "id": "Z-Bv6C7-1SRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(10)"
      ],
      "metadata": {
        "id": "y3tNkM7K11ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv('kh_sentiment_data_segmented.csv', index=False)"
      ],
      "metadata": {
        "id": "PywHg5UAjd3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[['seg_text_lstm', 'Sentiment_label']] # Use segmented text column and sentiment label\n",
        "\n",
        "# Split into train and validation sets\n",
        "train_data, val_data = train_test_split(data, test_size=0.2)"
      ],
      "metadata": {
        "id": "06p_LhtCDynF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "sLM5W68E-o3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the XLM-Roberta tokenizer\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")"
      ],
      "metadata": {
        "id": "jzZDEtXQ-oH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a customer dataset using Pytorch"
      ],
      "metadata": {
        "id": "FiaG46Pgo_UZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Custom Dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length):\n",
        "        self.dataframe = dataframe\n",
        "        self.texts = dataframe['seg_text_lstm'].tolist()\n",
        "        self.labels = dataframe['Sentiment_label'].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize the text\n",
        "        encoded = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        # Return tensors\n",
        "        return {\n",
        "            'input_ids': encoded['input_ids'].squeeze(0),  # Remove batch dimension\n",
        "            'attention_mask': encoded['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "jmV-ZHK676PL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters\n",
        "max_length = 128  # Maximum token length\n",
        "batch_size = 16   # Batch size\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = CustomDataset(train_data, tokenizer, max_length)\n",
        "val_dataset = CustomDataset(val_data, tokenizer, max_length)"
      ],
      "metadata": {
        "id": "SZrYq1Tp9B3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the first sample\n",
        "sample = train_dataset[0]\n",
        "# Print the tokenized data\n",
        "print(\"Input IDs:\", sample['input_ids'])\n",
        "print(\"Attention Mask:\", sample['attention_mask'])\n",
        "print(\"Label:\", sample['label'])"
      ],
      "metadata": {
        "id": "_u-WyOJcp0le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataloaders\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "5pXnLgxzqPuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Pre-train Model"
      ],
      "metadata": {
        "id": "9PfoRJ-oq-V8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class XLMRClassifier(nn.Module):\n",
        "    def __init__(self, model_name, num_classes):\n",
        "        super(XLMRClassifier, self).__init__()\n",
        "        self.xlm_roberta = XLMRobertaModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        # For binary classification, output 2 logits (for class 0 and 1)\n",
        "        self.fc = nn.Linear(self.xlm_roberta.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.xlm_roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Extract the [CLS] token representation\n",
        "        pooled_output = outputs[0][:, 0, :]\n",
        "\n",
        "        # Apply dropout and fully connected layer\n",
        "        x = self.dropout(pooled_output)\n",
        "        logits = self.fc(x)\n",
        "\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "Kr38uiiRqtdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "yuUHN2E5rPvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the model\n",
        "model = XLMRClassifier(model_name='xlm-roberta-base', num_classes=2)\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00001)\n",
        "criterion = torch.nn.CrossEntropyLoss()  # For classification tasks\n"
      ],
      "metadata": {
        "id": "xvo7-rkDrDJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a list to store loss history\n",
        "loss_history = []\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "# Training loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # tqdm for showing the progress bar\n",
        "    for batch in tqdm(train_data_loader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get input and output from the batch\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Get logits (the first element of the tuple)\n",
        "        logits = outputs\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Compute the average loss for this epoch\n",
        "    epoch_loss = running_loss / len(train_data_loader)\n",
        "    loss_history.append(epoch_loss)  # Save the loss for this epoch\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}\")"
      ],
      "metadata": {
        "id": "NAHH77HurT1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss history\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, epochs + 1), loss_history, marker='o', label='Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss History During Training')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0tfm-20xNWUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Lists to store predictions and true labels\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# Disable gradient computation for evaluation\n",
        "with torch.no_grad():\n",
        "    for batch in val_data_loader:\n",
        "        # Send inputs and labels to the device (GPU/CPU)\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Extract logits and compute predictions\n",
        "        logits = outputs\n",
        "        _, preds = torch.max(logits, dim=1)\n",
        "\n",
        "        # Store predictions and true labels\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds))\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(all_labels, all_preds))\n",
        "\n",
        "# Visualization of the confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Um9ccuyjs5IY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract probabilities for the positive class (class 1)\n",
        "all_probs = []\n",
        "\n",
        "# Disable gradient computation for evaluation\n",
        "with torch.no_grad():\n",
        "    for batch in val_data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        logits = outputs\n",
        "        probs = torch.softmax(logits, dim=1)  # Convert logits to probabilities\n",
        "        all_probs.extend(probs.cpu().numpy()[:, 1])  # Get probabilities for class 1\n",
        "\n",
        "# AUC Calculation\n",
        "fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Precision-Recall Curve\n",
        "precision, recall, _ = precision_recall_curve(all_labels, all_probs)\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='blue', lw=2)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.show()\n",
        "\n",
        "# Print AUC score\n",
        "print(f\"AUC: {roc_auc:.4f}\")"
      ],
      "metadata": {
        "id": "g7hqTy6cvF67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "mcc = matthews_corrcoef(all_labels, all_preds)\n",
        "print(f\"MCC: {mcc:.4f}\")\n"
      ],
      "metadata": {
        "id": "q70lNOvvyM6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the entire model\n",
        "model_save_path = \"xlmr_sentiment_model_full.pth\"\n",
        "torch.save(model, model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")\n"
      ],
      "metadata": {
        "id": "ZJ-3RCjodJju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pGoATG6y-wV3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}